{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione dati\n",
    "Questo notebook verrà usato per l'importazione di tutti i database che ci servono \\\n",
    "I dati saranno salvati quindi nella cartella data\\processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "#Per usare multi cartelle (cookiecutter)\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "\n",
    "import make_dataset as m_d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GRID\n",
    "grid=m_d.safe_import(\"grid\")\n",
    "\n",
    "grid.plot('cellId')\n",
    "grid\n",
    "\n",
    "## Non da' problemi, poi dovrò mergiarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questo fallisce ad importare, come mostrato a lezione 24\n",
    "\n",
    "tweets_json = json.load( open(m_d.data_path_in / m_d.files['twitter'][0]) )\n",
    "\n",
    "tweets = gpd.GeoDataFrame(tweets_json['features'])\n",
    "\n",
    "#Creiamo il punto smontando la casella point\n",
    "tweets['geometry'] = tweets['geomPoint.geom'].apply(lambda x:Point(x['coordinates'][0], x['coordinates'][1]))\n",
    "tweets.drop(columns=['geomPoint.geom'],inplace=True)\n",
    "\n",
    "#Droppo roba inutile\n",
    "tweets.drop(columns=['municipality.acheneID'],inplace=True)\n",
    "tweets.drop(columns=['entities'],inplace=True)\n",
    "\n",
    "tweets.plot(\"municipality.name\")\n",
    "\n",
    "#tweets.dtypes\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Fallisce ad importare\n",
    "weather=m_d.safe_import(\"weather\")\n",
    "weather\n",
    "#Come prima ho metadati\n",
    "\"\"\"\n",
    "weather_json = json.load( open(m_d.data_path_in / m_d.files['weather'][0]) )\n",
    "weather = gpd.GeoDataFrame(weather_json['features'])\n",
    "\n",
    "\n",
    "#Elimino le colonne del vento (dati molto incompleti)\n",
    "weather.drop(weather.columns[list(range(202,298))], axis=1, inplace=True)\n",
    "weather.drop(columns=['minWind', \"maxWind\"], inplace=True)\n",
    "\n",
    "#Svolgiamo infine i punti geometrici\n",
    "weather['geometry'] = weather['geomPoint.geom'].apply(lambda x:Point(x['coordinates'][0], x['coordinates'][1]))\n",
    "weather.drop(columns=['geomPoint.geom'],inplace=True)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Lo salvo che mi serve in sezione ML\n",
    "weather.to_csv(m_d.data_path_out / 'weather_final.csv',index=False)\n",
    "\n",
    "weather.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = grid.plot(color=\"orange\")\n",
    "weather.plot(column='maxTemperature',ax=ax)\n",
    "\n",
    "#Visto che i tweets sono disassociati dalle stazioni, risulta necessario restringersi alla stazione più vicina\n",
    "# -> Creo database delle stazioni\n",
    "stations=m_d.orderstation(weather)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Rimane un problema, le temperature non sono svolte\n",
    "Spoiler: svolgerle è molto difficile (pandas ha davvero forte limitazioni sulle azioni che fa fare)\n",
    "         le soluzioni sono molto non-pythoniche (o almeno quelle che ho trovato)\n",
    "Idea migliore: creo una funzione (timesearch) che svolge il tempo per fare in modo che mi trovi temperatura e precipitazioni\n",
    "\"\"\"\n",
    "stations.head(5)\n",
    "print(\"Mappa della distibuzione della stazioni sul territorio trentino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precipitazioni\n",
    "Questa tabella è **probabilmente** più precisa di weather (fornisce per ogni elemento della griglia) -> darebbe precipitazione più precisa, ma mancano molte entries. Per ora usiamo solo quello del weather (anche perchè ho già scritto tutto con quel database) \\\n",
    "NOTA: per far questo basta un merge con DB tweets, ma devo ricalcolare il valore alle mezz'ore a causa di come ho discretizzato sotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lascio qua la cella a puro scopo di chiarezza\n",
    "\"\"\"\n",
    "Questo si comporta bene\n",
    "Non è utile importare i data availability tendenzialmente\n",
    "La seconda colonna è il cellId, non viene detto ma si può verificare con il database grid\n",
    "    suppongo la terza colonna sia intensità\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "dummy=m_d.safe_import('precip')\n",
    "#Problema: il primo dato vien preso come dizionario\n",
    "#Converto colonne\n",
    "dummy=dummy.rename(columns={'201311010000': 'time', \"2383\": 'cellId', \"1\" : 'intensity'})\n",
    "\n",
    "#Riaggiungo il primo dato\n",
    "precipitation=m_d.appforth(dummy,[201311010000,2383,1])\n",
    "\n",
    "#Converto il tempo per averlo in formato identico agli altri\n",
    "temp=np.array(precipitation[\"time\"])\n",
    "#Questi sono integers, li svolgo in modo diverso\n",
    "minutes=temp%100\n",
    "temp=np.floor(temp/100)\n",
    "precipitation[\"hours\"]=np.around(temp%100+minutes/60,2)\n",
    "temp=np.floor(temp/100)\n",
    "precipitation[\"day\"]=[int(i%100) for i in temp]\n",
    "temp=np.floor(temp/100)\n",
    "precipitation[\"month\"]=[int(i%100) for i in temp]\n",
    "precipitation.drop(columns=[\"time\"], inplace=True)\n",
    "\n",
    "precipitation\n",
    "\"\"\"\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# devo prima unire i due dataset di line set -> nov e poi nov -> dicembre\n",
    "\n",
    "#Problema: il primo dato vien preso come dizionario\n",
    "electro = m_d.safe_import('SET-1')\n",
    "electro=electro.rename(columns={'DG1000420': 'LINESET', \"2013-11-01 00:00\": 'Timestamp', \"37.439999\" : 'Value Amp'})\n",
    "#Riaggiungo il primo dato\n",
    "electro=m_d.appforth(electro,['DG1000420','2013-11-01 00:00',37.439999])\n",
    "\n",
    "temp = m_d.safe_import('SET-2')\n",
    "temp=temp.rename(columns={'DG1000420': 'LINESET', \"2013-12-01 00:00\": 'Timestamp', \"36.719997\" : 'Value Amp'})\n",
    "#Riaggiungo il primo dato\n",
    "temp=m_d.appforth(temp,['DG1000420','2013-12-01 00:00',36.719997])\n",
    "\n",
    "electro=electro.append(temp, ignore_index=True)\n",
    "del temp  #Pesa tanto\n",
    "\n",
    "electro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unisco a dataframe che descrive posizioni sulla griglia\n",
    "#Ne faccio un secondo che per alcune operazione è più comodo operare con l'altro\n",
    "lines = m_d.safe_import('SET-lines')\n",
    "electroLines = electro.merge(right=lines, how='outer')\n",
    "\n",
    "#Ci sono incompatibilità tra i record salvati quindi li droppo cattivo\n",
    "electroLines = electroLines.dropna()\n",
    "\n",
    "electroLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Serve svolgere il timestamp\n",
    "temp=list(electro[\"Timestamp\"])\n",
    "electro[\"month\"]=[int(st[5:7]) for st in temp]\n",
    "electro[\"day\"]=[int(st[8:10]) for st in temp]\n",
    "#electro[\"hours\"]=[int(st[11:13])+int(st[14:16])/60 for st in temp]\n",
    "electro[\"hours\"]=[np.around(int(st[11:13])+int(st[14:16])/60, 2) for st in temp]\n",
    "    #Round per safer groupby, albeit slower (should not matter, just print uniques)\n",
    "    #Utile che fa anche un file più leggero\n",
    "#print(electro.hours.unique())\n",
    "    \n",
    "electro.drop(columns=[\"Timestamp\"], inplace=True)\n",
    "electro.to_csv(m_d.data_path_out / 'electro_final.csv',index=False)\n",
    "    #Lo salvo che mi serve altrove, e questo è lenta da eseguire\n",
    "\n",
    "electro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIRCOSCRIZIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Friendship ended with database del prof, database della provincia is now my best friend\n",
    "circ=m_d.safe_import('circoscrizioni')\n",
    "\n",
    "circ.plot(\"area\")\n",
    "circ=circ.sort_values(\"numero_cir\").reset_index()\n",
    "circ.drop(columns=[\"index\", \"numero_cir\"], inplace=True)\n",
    "circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sistemo per riallacciare\n",
    "circ.drop(columns=[\"fumetto\", \"area\", \"perimetro\"],inplace=True)\n",
    "circ.rename(columns={\"nome\" : \"circoscrizione\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unione di Databases\n",
    "Passo 1: creare un database dei tweets; a ogni tweet associo tempo atmosferico e temperatura \\\n",
    "\n",
    "Il tempo viene binnato in 30 minuti (mediamente abbiamo sui 20 tweets all'ora), visto che i dati sono ottenuti ogni 10 o 15 perderei informazioni ma per i tweets uso diretto il valore dalla tabella)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets=tweets.set_crs(\"EPSG:4326\")\n",
    "circ=circ.to_crs(\"EPSG:4326\")\n",
    "tweets = gpd.sjoin(tweets, grid, how=\"inner\", op='intersects')\n",
    "tweets.drop(columns=\"index_right\", inplace=True)\n",
    "tweets = gpd.sjoin(tweets, circ, how=\"left\", op='intersects') \n",
    "    #Questi li attacco anche se potrei usare dirretto la grid, così è più comodo\n",
    "tweets.drop(columns=\"index_right\", inplace=True)\n",
    "tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visto che l'ora di tempo non è una quantità che correla a qualcosa di ciclico effettivamente misurabile*, ha poco senso\n",
    "#separare ore e minuti ed quindi tanto vale trattarli insieme\n",
    "#*Assumeremo eventi quali la campanella dei ragazzi delle superiori non influenzi in maniera importante i dati\n",
    "\n",
    "###TEMPO DI ESECUZIONE QUALCHE MIN\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "Tw_final=pd.DataFrame()\n",
    "\n",
    "\n",
    "N=len(tweets[\"created\"])  #Numero tweets\n",
    "temp=tweets[\"created\"]\n",
    "#TEMPO va smontato\n",
    "Tw_final[\"month\"]=[int(st[5:7]) for st in temp]\n",
    "Tw_final[\"day\"]=[int(st[8:10]) for st in temp]\n",
    "Tw_final[\"hours\"]=[int(st[11:13])+0.5*(int(st[14:16])>=30) for st in temp]\n",
    "\n",
    "# TEMPERATURA E PRECIPITAZIONI\n",
    "# per ricavare queste usiamo la stazione più vicina al tweet\n",
    "# (sfortunamente il Trentino non ne ha tante, fortunatamente sono distribuite bene)\n",
    "    \n",
    "#Voglio farlo pythonico e rapido, quindi si userà nearest point\n",
    "    #(ironicamente builtin sembra più lenta, si può ottimizzare ulteriormente)\n",
    "#Vedi notebook note informative per una very very basic overview\n",
    "nearest=[ stations[stations[\"geometry\"]==nearest_points(gm,\n",
    "            gpd.GeoSeries(stations[\"geometry\"]).unary_union)[1]][\"station\"].values[0] for gm in tweets[\"geometry\"]]\n",
    "    #lista con nome della stazione più vicina\n",
    "\n",
    "T=[]\n",
    "R=[]\n",
    "#Questo è ancora migliorabile come forma\n",
    "for i in range(0,N):\n",
    "    T.append(m_d.find_Weather(weather, Tw_final.loc[i][\"month\"],\n",
    "                                Tw_final.loc[i][\"day\"], Tw_final.loc[i][\"hours\"], nearest[i], varType=0))\n",
    "    R.append(m_d.find_Weather(weather, Tw_final.loc[i][\"month\"],\n",
    "                                Tw_final.loc[i][\"day\"], Tw_final.loc[i][\"hours\"], nearest[i], varType=1))\n",
    "Tw_final[\"temperature\"]=T\n",
    "Tw_final[\"rain\"]=R\n",
    "\n",
    "\n",
    "#Il resto lo importo direttamente\n",
    "Tw_final[\"municipal\"]=tweets[\"municipality.name\"]\n",
    "Tw_final[\"cellId\"]=tweets[\"cellId\"]\n",
    "Tw_final[\"language\"]=tweets[\"language\"]\n",
    "Tw_final[\"circoscrizione\"]=tweets[\"circoscrizione\"]\n",
    "\n",
    "#Salviamo infine il database\n",
    "Tw_final.to_csv(m_d.data_path_out / 'twitter_final.csv',index=False)\n",
    "\n",
    "############MANCA ELETTRICITA' A QUESTO FILE (SE VOGLIO FARE EDA Ntw vs Elett)!!!!!!!!!!!\n",
    "\n",
    "Tw_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roba che è meglio checckare\n",
    "#Tw_final[Tw_final[\"temperature\"].isnull()]  #Pochi tweets senza valore, poco male\n",
    "Tw_final[Tw_final[\"circoscrizione\"].isnull()]  #8000 tweets a trento, ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggiungiamo anche il consumo elettrico associato al tweet\n",
    "    #prob overkill per l'EDA ma dovrei saperlo fare in fretta ormai \n",
    "#Tw_final=pd.read_csv(m_d.data_path_out / \"twitter_final.csv\")\n",
    "\n",
    "#ElectroLines.groupby(\"SQUAREID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "Voglio creare un database su cui operare di machine learning \\\n",
    "Uso database salvati con appropriate assunzioni \\\n",
    "Lo faccio in funzione che è molto lunga e non ha c'è molto da presentare \\\n",
    "Vedi meglio in sezione ML \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressdB=m_d.df_reg()\n",
    "regressdB.to_csv(m_d.data_path_out / 'MachineLearningDB.csv',index=False)\n",
    "\n",
    "regressdB.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
