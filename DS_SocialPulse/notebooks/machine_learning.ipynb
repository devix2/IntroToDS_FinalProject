{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfb0f1e",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "Tutta la parte dedicata al machine learning (regressione + classificazione) verrà scritta qui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b05087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# custom lib\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "\n",
    "import make_dataset as m_d\n",
    "import ML_functions as supp_ML\n",
    "\n",
    "np.random.seed(86122330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030195b",
   "metadata": {},
   "source": [
    "# Prima task: prevedere l'attività di twitter a livello provinciale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4c2d242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet1m</th>\n",
       "      <th>Tweet2m</th>\n",
       "      <th>Tweet1n</th>\n",
       "      <th>Tweet2n</th>\n",
       "      <th>Tavg1m</th>\n",
       "      <th>Tavg2m</th>\n",
       "      <th>Tavg1n</th>\n",
       "      <th>Tavg2n</th>\n",
       "      <th>Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>165</td>\n",
       "      <td>44</td>\n",
       "      <td>92</td>\n",
       "      <td>11.245556</td>\n",
       "      <td>12.132153</td>\n",
       "      <td>9.895139</td>\n",
       "      <td>10.252631</td>\n",
       "      <td>Su</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>149</td>\n",
       "      <td>74</td>\n",
       "      <td>44</td>\n",
       "      <td>12.137079</td>\n",
       "      <td>11.245556</td>\n",
       "      <td>7.764167</td>\n",
       "      <td>9.895139</td>\n",
       "      <td>Mo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>115</td>\n",
       "      <td>54</td>\n",
       "      <td>74</td>\n",
       "      <td>7.444653</td>\n",
       "      <td>12.137079</td>\n",
       "      <td>6.732540</td>\n",
       "      <td>7.764167</td>\n",
       "      <td>Tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>10.234794</td>\n",
       "      <td>7.444653</td>\n",
       "      <td>6.847639</td>\n",
       "      <td>6.732540</td>\n",
       "      <td>We</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>106</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>10.780688</td>\n",
       "      <td>10.234794</td>\n",
       "      <td>6.581250</td>\n",
       "      <td>6.847639</td>\n",
       "      <td>Th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88</td>\n",
       "      <td>102</td>\n",
       "      <td>92</td>\n",
       "      <td>55</td>\n",
       "      <td>10.987268</td>\n",
       "      <td>10.780688</td>\n",
       "      <td>6.968611</td>\n",
       "      <td>6.581250</td>\n",
       "      <td>Fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet1m  Tweet2m  Tweet1n  Tweet2n     Tavg1m     Tavg2m    Tavg1n  \\\n",
       "0      149      165       44       92  11.245556  12.132153  9.895139   \n",
       "1      115      149       74       44  12.137079  11.245556  7.764167   \n",
       "2       90      115       54       74   7.444653  12.137079  6.732540   \n",
       "3      106       90       56       54  10.234794   7.444653  6.847639   \n",
       "4      102      106       55       56  10.780688  10.234794  6.581250   \n",
       "5       88      102       92       55  10.987268  10.780688  6.968611   \n",
       "\n",
       "      Tavg2n Weekday  \n",
       "0  10.252631      Su  \n",
       "1   9.895139      Mo  \n",
       "2   7.764167      Tu  \n",
       "3   6.732540      We  \n",
       "4   6.847639      Th  \n",
       "5   6.581250      Fr  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importo i dati dal database finale (vedi Import Dati)\n",
    "data = pd.read_csv(m_d.data_path_out / 'MachineLearningDB.csv')\n",
    "\n",
    "# Smisto target e features\n",
    "target_mattina = data['TargetDay']\n",
    "target_sera = data['TargetNight']\n",
    "data.drop(columns=['TargetDay', 'TargetNight'], inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#### PROBLEMA: questo encodda il weekday ma non lo svolge in colonne\n",
    "#### (non è bello svolgerlo, e poco generale maybe)\n",
    "# Encoder sul dato categorico weekday\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "data[\"Weekday\"]=enc.fit_transform(data[\"Weekday\"].values[:,None]).tolist()\n",
    "\n",
    "NOTA: se inserisco un np.array(np.array) ad un dataframe MI TIENE SOLO PRIMO ELEMENTO (1.0<=>'Mo')\n",
    " Se converto a lista funziona\n",
    " \n",
    "Nota: equivale a\n",
    "data['Weekday'] = enc.fit(data[\"Weekday\"].values[:,None])\n",
    "data['Weekday'] = enc.transform(data[\"Weekday\"].values[:,None]).tolist()\n",
    "\n",
    "Nota: Per convertire i contenuti a np.array(), ma comunque non fa funzionare sklearn\n",
    "temp=enc.fit_transform(data[\"Weekday\"].values[:,None])\n",
    "data[\"Weekday\"]=[np.array(i) for i in temp]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Facendo make_column_transformer funziona, ma meglio inserirlo nella pipeline\n",
    "colTr = make_column_transformer( (StandardScaler(), [\"Tweet1m\", \"Tweet2m\", \"Tweet1n\", \"Tweet2n\",\n",
    "                                                     \"Tavg1m\", \"Tavg2m\", \"Tavg1n\", \"Tavg2n\"]),\n",
    "                                (OneHotEncoder(), ['Weekday']), remainder='passthrough')\n",
    "\n",
    "\n",
    "#Restringere database di features, molte delle features ipotizzate contano poco, ed ho molti pochi dati\n",
    "data=data[[\"Tweet1m\", \"Tweet2m\", \"Tweet1n\", \"Tweet2n\", \"Tavg1m\", \"Tavg2m\", \"Tavg1n\", \"Tavg2n\", \"Weekday\"]]\n",
    "\n",
    "#enc=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "data=colTr.fit_transform(data)\n",
    "data\n",
    "\"\"\"\n",
    "num_feat=[\"Tweet1m\", \"Tweet2m\", \"Tweet1n\", \"Tweet2n\", \"Tavg1m\", \"Tavg2m\", \"Tavg1n\", \"Tavg2n\"]\n",
    "cat_feat=[\"Weekday\"]\n",
    "\n",
    "data[num_feat+cat_feat].head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3da84e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression r2_score = 0.22838043193849\n",
      "Random forest r2_score =  0.38384423651130983\n",
      "Random forest (gridSearchCV) r2_score =  0.4881223453408208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('standardscaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['Tweet1m', 'Tweet2m',\n",
       "                                                   'Tweet1n', 'Tweet2n',\n",
       "                                                   'Tavg1m', 'Tavg2m', 'Tavg1n',\n",
       "                                                   'Tavg2n']),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(),\n",
       "                                                  ['Weekday'])])),\n",
       "                ('Regressor', RandomForestRegressor(bootstrap=False))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MATTINA:\n",
    "supp_ML.logistic_regressor_fittato(data, target_mattina, num_feat, cat_feat)\n",
    "supp_ML.Random_Forest_Regressor_CV(data, target_mattina, num_feat, cat_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ac70f",
   "metadata": {},
   "source": [
    "# Osservazioni:\n",
    "1) Altissima varianza valore r2, ho molti pochi dati per fare questa stima (also, ci sono i dati del fine anno e delle feste che sballano tutto) \\\n",
    "2) Rimuovere features aiuta abbastanza \\\n",
    "3) Includere il weekday **sembra** peggiorare la stima (probabilmente dovuto al fatto che finchè non lo fixiamo il one-hot-encoder hitta alcune colonne di numeri interi\n",
    "\n",
    "Overall sembra che il predittore sia better-than random usando come features solo numero di tweets dei giorni precendenti, ma si trova comunque lontano da un buon predittore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb3f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest r2_score =  0.12764175814188095\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
      "Random forest r2_score =  0.13423090662168358\n",
      "Random forest r2_score =  0.02151156262086995\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
      "Random forest r2_score =  0.03665729974921428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('encoder',\n",
       "                 OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('Regressor', RandomForestRegressor(bootstrap=False))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "supp_ML.Random_Forest_Regressor_CV(data, target_mattina)\n",
    "supp_ML.Random_Forest_Regressor_CV(data, target_sera)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543c2a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA', 'OLTREFERSINA', 'OLTREFERSINA', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO', 'CENTRO STORICO PIEDICASTELLO']\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
      "Lo score della nostra Random Forest risulta essere: -0.5555555555555556 per il riconoscimento delle circoscrizioni più attive\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adesso mi occupo di fare la parte di classificazione: voglio individuare quali siano le circoscrizioni di Trento che hanno\n",
    "il più alto numero di interazioni sociali basandomi sui dati delle giornate precedenti. \n",
    "Il dataset sarà uguale a quello precedente con la differenza che la parte di conteggi delle X sarà divisa per circoscrizione\n",
    "Dal momento che ci sono 12 circoscrizioni i parametri di input saranno \n",
    "[interazioni sociali giorno i-2 divisi per circoscrizione][interazioni sociali giorno i-1 divisi per circoscrizione] + altri parametri\n",
    "Per quanto riguarda i target, considereremo la circoscrizione con il maggior numero di tweets.\n",
    "Inizialmente provo a lavorare con un Classificatore random forest, dal momento che abbiamo pochi elementi\n",
    "\"\"\"\n",
    "# Random Forest Classifier\n",
    "supp_ML.Random_Forest_Classifier_Circoscrizione(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a621a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59accfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c527f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e196fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
