{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfb0f1e",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "Tutta la parte dedicata al machine learning (regressione + classificazione) verrà scritta qui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b05087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# custom lib\n",
    "import sys\n",
    "sys.path.append('./../src')\n",
    "\n",
    "import make_dataset as m_d\n",
    "import ML_functions as supp_ML\n",
    "\n",
    "np.random.seed(86122330)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030195b",
   "metadata": {},
   "source": [
    "# PRIMA TASK: prevedere l'attività di twitter a livello provinciale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c2d242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet1m</th>\n",
       "      <th>Tweet2m</th>\n",
       "      <th>Tweet1n</th>\n",
       "      <th>Tweet2n</th>\n",
       "      <th>Tavg1m</th>\n",
       "      <th>Tavg2m</th>\n",
       "      <th>Tavg1n</th>\n",
       "      <th>Tavg2n</th>\n",
       "      <th>Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>165</td>\n",
       "      <td>44</td>\n",
       "      <td>92</td>\n",
       "      <td>11.245556</td>\n",
       "      <td>12.132153</td>\n",
       "      <td>9.895139</td>\n",
       "      <td>10.252631</td>\n",
       "      <td>Su</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>149</td>\n",
       "      <td>74</td>\n",
       "      <td>44</td>\n",
       "      <td>12.137079</td>\n",
       "      <td>11.245556</td>\n",
       "      <td>7.764167</td>\n",
       "      <td>9.895139</td>\n",
       "      <td>Mo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>115</td>\n",
       "      <td>54</td>\n",
       "      <td>74</td>\n",
       "      <td>7.444653</td>\n",
       "      <td>12.137079</td>\n",
       "      <td>6.732540</td>\n",
       "      <td>7.764167</td>\n",
       "      <td>Tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>10.234794</td>\n",
       "      <td>7.444653</td>\n",
       "      <td>6.847639</td>\n",
       "      <td>6.732540</td>\n",
       "      <td>We</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>106</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>10.780688</td>\n",
       "      <td>10.234794</td>\n",
       "      <td>6.581250</td>\n",
       "      <td>6.847639</td>\n",
       "      <td>Th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88</td>\n",
       "      <td>102</td>\n",
       "      <td>92</td>\n",
       "      <td>55</td>\n",
       "      <td>10.987268</td>\n",
       "      <td>10.780688</td>\n",
       "      <td>6.968611</td>\n",
       "      <td>6.581250</td>\n",
       "      <td>Fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet1m  Tweet2m  Tweet1n  Tweet2n     Tavg1m     Tavg2m    Tavg1n  \\\n",
       "0      149      165       44       92  11.245556  12.132153  9.895139   \n",
       "1      115      149       74       44  12.137079  11.245556  7.764167   \n",
       "2       90      115       54       74   7.444653  12.137079  6.732540   \n",
       "3      106       90       56       54  10.234794   7.444653  6.847639   \n",
       "4      102      106       55       56  10.780688  10.234794  6.581250   \n",
       "5       88      102       92       55  10.987268  10.780688  6.968611   \n",
       "\n",
       "      Tavg2n Weekday  \n",
       "0  10.252631      Su  \n",
       "1   9.895139      Mo  \n",
       "2   7.764167      Tu  \n",
       "3   6.732540      We  \n",
       "4   6.847639      Th  \n",
       "5   6.581250      Fr  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importo i dati dal database finale (vedi Import Dati)\n",
    "data = pd.read_csv(m_d.data_path_out / 'MachineLearningDB.csv')\n",
    "\n",
    "# Smisto target e features\n",
    "target_mattina = data['TargetDay']\n",
    "target_sera = data['TargetNight']\n",
    "data.drop(columns=['TargetDay', 'TargetNight'], inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#### PROBLEMA: questo encodda il weekday ma non lo svolge in colonne\n",
    "#### (non è bello svolgerlo, e poco generale maybe)\n",
    "# Encoder sul dato categorico weekday\n",
    "enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "data[\"Weekday\"]=enc.fit_transform(data[\"Weekday\"].values[:,None]).tolist()\n",
    "\n",
    "NOTA: se inserisco un np.array(np.array) ad un dataframe MI TIENE SOLO PRIMO ELEMENTO (1.0<=>'Mo')\n",
    " Se converto a lista funziona\n",
    "\n",
    "Nota: equivale a\n",
    "data['Weekday'] = enc.fit(data[\"Weekday\"].values[:,None])\n",
    "data['Weekday'] = enc.transform(data[\"Weekday\"].values[:,None]).tolist()\n",
    "\n",
    "Nota: Per convertire i contenuti a np.array(), ma comunque non fa funzionare sklearn\n",
    "temp=enc.fit_transform(data[\"Weekday\"].values[:,None])\n",
    "data[\"Weekday\"]=[np.array(i) for i in temp]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Facendo make_column_transformer funziona, ma meglio inserirlo nella pipeline\n",
    "colTr = make_column_transformer( (StandardScaler(), [\"Tweet1m\", \"Tweet2m\", \"Tweet1n\", \"Tweet2n\",\n",
    "                                                     \"Tavg1m\", \"Tavg2m\", \"Tavg1n\", \"Tavg2n\"]),\n",
    "                                (OneHotEncoder(), ['Weekday']), remainder='passthrough')\n",
    "\n",
    "\n",
    "#Restringere database di features, molte delle features ipotizzate contano poco, ed ho molti pochi dati\n",
    "data=data[[\"Tweet1m\", \"Tweet2m\", \"Tweet1n\", \"Tweet2n\", \"Tavg1m\", \"Tavg2m\", \"Tavg1n\", \"Tavg2n\", \"Weekday\"]]\n",
    "\n",
    "#enc=OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "data=colTr.fit_transform(data)\n",
    "data\n",
    "\"\"\"\n",
    "num_feat=[\"Tweet1m\", \"Tweet2m\", \"Tweet1n\", \"Tweet2n\", \"Tavg1m\", \"Tavg2m\", \"Tavg1n\", \"Tavg2n\"]\n",
    "cat_feat=[\"Weekday\"]\n",
    "\n",
    "data[num_feat+cat_feat].head(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da84e7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression r2_score = 0.3928198052884144\n",
      "Random forest r2_score =  0.4729625222709424\n",
      "Random forest (gridSearchCV) r2_score =  0.6117988705391918\n"
     ]
    }
   ],
   "source": [
    "#MATTINA:\n",
    "PL_Matt_Logistic=supp_ML.logistic_regressor_fittato(data, target_mattina, num_feat, cat_feat)\n",
    "PL_Matt_RF=supp_ML.Random_Forest_Regressor_CV(data, target_mattina, num_feat, cat_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df2da8",
   "metadata": {},
   "source": [
    "Nota: se tolgo handle_unknown=\"ignore\" dal one hot encoder raisa un messaggio di errore a volte (dipende dal random seed del problema), il seed 86122330 attiva il problema (eseguendo il notebook in ordine). \\\n",
    "Vista la soluzione al problema si penserebbe che sia un problema del one hot encoder, ma non vedo perchè dovrebbe **a volte** fallire. L'unica cosa che ho pensato era \"forse nell'insieme di train non ci sono tutti i giorni della settimana\", ma printare\n",
    "```python\n",
    "PL_Matt_RF[\"transformer\"].transformers_[1][1].categories_\n",
    "```\n",
    "oppure (se uso il gridsearch)\n",
    "```python\n",
    "PL_Matt_RF.best_estimator_[\"transformer\"].transformers_[1][1].categories_\n",
    "```\n",
    "dimostra che questo non è il problema. Il warning è raisato nella parte di CV durante il fit \\\n",
    "Nota: l'errore contiene 432 NaNs, che sono i 432 candidati della gridsearch (in teoria); non so se implichi qualcosa questo \\\n",
    "Nota: se faccio l'ignore aumenta drasticamente l'r2, probabilmente perchè aumentano di molto i punti di training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ac70f",
   "metadata": {},
   "source": [
    "## Osservazioni:\n",
    "**Regressione:** altissima varianza valore r2, ho molti pochi dati per fare questa stima (also, ci sono i dati del fine anno e delle feste che sballano tutto) \\\n",
    "**Random Forest:** piuttosto stabile nonostante lo scarso numero di dati (as predicted), sicuramente better-than-random ma visto il basso numero di dati non riscontriamo successi strepitosi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb3f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression r2_score = -0.2739447175057743\n",
      "Random forest r2_score =  -0.06609060313739756\n",
      "Random forest (gridSearchCV) r2_score =  0.4970140696718709\n"
     ]
    }
   ],
   "source": [
    "#SERA\n",
    "PL_Sera_Logistic=supp_ML.logistic_regressor_fittato(data, target_sera, num_feat, cat_feat)\n",
    "PL_Sera_RF=supp_ML.Random_Forest_Regressor_CV(data, target_sera, num_feat, cat_feat)\n",
    "\n",
    "#More of the same, niente di strano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9f70a",
   "metadata": {},
   "source": [
    "# SECONDA TASK: identificare la circoscrizione con più tweets associati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e8f2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CENTRO STORICO PIEDICASTELLO', 'OLTREFERSINA'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creo il vettore delle y trovando qual è la circoscrizione più attiva\n",
    "targetCirc = supp_ML.circoscrizione_attiva(m_d.data_path_out / \"twitter_final.csv\")\n",
    "targetCirc = pd.Series(targetCirc)\n",
    "targetCirc.drop([targetCirc.index[0], targetCirc.index[1]], inplace=True)\n",
    "targetCirc.unique()  #Ho solo 2 valori rilevanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543c2a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo score della nostra Random Forest risulta essere: 0.8333333333333334 per il riconoscimento delle circoscrizioni più attive\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "PL_classPred=supp_ML.Random_Forest_Classifier_Circoscrizione(data, targetCirc, num_feat, cat_feat)\n",
    "\n",
    "#~75% di accuracy non fa neanche troppo schifo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a621a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59accfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c527f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e196fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
